<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[About Matrix]]></title>
      <url>/linear_algebra/silly_questions/2022/03/28/About-Matrix/</url>
      <content type="text"><![CDATA[행렬 첫번째 시리즈를 소개한다.1월부터 틈틈이 선형대수 공부를 해오다 1월초에 SVD, 얼마전 eigen decomposition과 power iteration을 정리하면서 eigenvalue와 eigenvector에 대해 고민을 많이 했다. 행렬 A를 곱하더라도 방향이 바뀌지 않는 벡터는 eigenvector다. 즉, 곱하더라도 방향은 안바뀌고 원래의 방향으로 늘어나거나 줄어들기만 한다. 그 벡터에 행렬 A를 곱했을 때 scaling 되는 정도가 eigenvalue다. 라는 정의만으로는 덜 와닿았던 것 같고,3Blue1Brown 영상을 보고 기하적으로 이해는 조금 더 됐는데.. 친구가 “그래서 왜 중요한데?” 라는 질문에는 결국 제대로 설명을 못하겠더라. 그때 나는 power iteration method 을 공부 하고 있었는데, 이걸로 eigenvector의 중요성을 설명하려다 보니 닭과 계란이 되는 느낌이었다.그래서 양자컴퓨팅 과제하던 그 친구랑 “why eigenvector matters” 등등으로 검색하면서 행렬과 eigenvector 연관성 검색을 한참했다. 직관적인 설명을 찾고 싶었는데 생각보다 찾긴 어려웠고 그나마 수확이라면, 엄밀하게는 수학 정의에 어긋난 부분이 있는 것 같지만 그래도 그나마 설명을 잘한 글을 얻었다 정도? 결론은 둘다 할 일 제대로 못하고 서브웨이 먹으러 갔다.생각해보면 학부 때 선형대수를 별로 안좋아했는데 그건 내가 고등학교 때 기하와 벡터는 좋아했지만 행렬은 안좋아했던 이유가 크다. 참, 거짓을 찾는 문제 유형도 별로였고 그 중의 반 정도는 반례를 찾아야하는게 썩 재미있진 않았다.그런데 요새 행렬을 보면서 재미있다고 느끼는걸 보면 충분히 왜라는 질문 없이 받아들이기만 하고, 또 행렬이 어떻게 쓰이는지도 몰라서였던게 큰 것 같다.여하튼 이번에는 PCA 공부하려다 공돌이 수학노트 분의 영상을 보게됐는데 꽤 재미있고, 너무 당연하게만 받아들였거나 새로운 해석이라 싶은 부분이 있어서 남겨보려한다. 그리고 오늘 읽은 WHEN VISION TRANSFORMERS OUTPERFORM RESNETS WITHOUT PRE-TRAINING OR STRONG DATA AUGMENTATIONS 에서 어떻게 ViT, ResNet, MLP mixer의 generalization 분석을 위해 선형대수적 특징을 가져와서 사용하는지도 짧게 남겨본다. 블로그 latex 기능을.. 안살펴봐서 수식이 어그러진 글이 될 것 같다.(참고) power iteration method : 큰 차원의 행렬이 가진 모든 eigenvalue를 찾기는 컴퓨팅 측면에서 어려우니(또는 굳이 다 찾을 필요가 없으니) 가장 큰 eigenvalue를 찾는 알고리즘이다. 임의의 벡터 x에 dominant eigenvalue 값이 있는 행렬 A(행렬에 n 개의 eigenvalue 가 있을때 압도적으로 크다고 볼 수 있는 eigenvalue)를 여러번 곱해주면 벡터 x는 결국 A의 dominant eigenvalue에 대응되는 eigenvector 방향으로의 벡터로 변환된다. 이 성질을 이용해 반복적으로 A를 곱하고, 큰 원소값으로 normalize 해주면서 eigenvalue 찾는 법이다. 더 자세한 사항은 책이나 페이지를 찾아보자.행렬의 의미먼저 행렬에 대해 생각해보자. 크게 두가지 측면으로 보면 좋은데  데이터 뭉터기 : 다차원 feature를 가진 데이터들의 집합으로 생각한다면 행렬은 데이터 뭉터기를 표현할 수 있다.  선형 변환 : kx1 크기의 벡터에 m x k 행렬을 곱하는 것은 k 차원에 있는 벡터를 m 차원으로 projection 시켜주는 것인데,  이때의 projection 의미를 벡터가 존재하는 축을 회전 또는 scaling해서 변환하는 것으로 볼 수 있다.행렬곱의 의미그럼 벡터에 행렬을 곱해준다는 것은 어떤 의미일까?  행렬 곱 AB를 생각하면 A의 행벡터와 B의 열벡터의 내적으로 연산하는 것이다.   보통 벡터 는 열벡터를 의미하고 행벡터는 열벡터에 대한 함수라고 본다.   다시말해서 열벡터는 변화의 대상이 되는 함수의 입력값이고 행벡터는 변화를 시키는 operation 이다.  결국 행벡터는 열벡터를 입력으로 받아 스칼라를 출력하는 함수, functional이 된다고 볼 수 있겠다.          이렇게 행벡터와 열벡터간의 내적을 한다는 고유의 의미를 살린 응용은 공분산 행렬에 적용된다. 공분산 행렬은 데이터 간 닮은 정도를 표현하는 행렬이다. 더 자세한 내용은 다음에 PCA를 다루면서 설명하겠다.        행렬과 벡터의 곱은 행렬과 열벡터를 선형결합 형태로 표현하는 것이다.          벡터의 선형결합이 중요한 것은 새로운 벡터 공간을 생성해줄 수 있다는 것인데, 두 개의 열벡터[1 3]^T 와 [2 4]^T 는 [3 5]^T 벡터를 포함하는 벡터 공간을 생성해낼 수 있는가? 만약 그렇다면 어떻게 두 벡터를 조합하면 될까? 라고 이어져서 생각할 수 있다.      이런 해석은 선형 연립 방정식의 해, 회귀 분석의 계수를 찾는 과정에서 행렬 곱을 바라보는 시선이 되겠다.        2번의 해석을 생각해보면 행렬이 벡터의 선형 변해줬다고 볼 수 있는데 이 부분은 위의 행렬에 대한 생각 2번과 일치하기 때문에 생략한다.          이런 관점으로 행렬을 보면 eigenvalue&amp;eigenvector, PCA, SVD 등으로 응용될 수 있다.      특히 1번과 3번을 합쳐서 생각하면, 행렬과 벡터의 곱에서 행렬은 선형변환이라는 일종의 함수 역할을 하게 되는데 그렇다면 선형변환은 함수 가 가지는 정의를 만족하는가?에 대한 고민이 좀더 필요하다.wiki에서의 함수 정의는 In mathematics, a function from a set X to a set Y assigns to each element of X exactly one element of Y 로 정의역의 각 원소를 정확히 하나의 공역 원소에 대응시키는 것이다. f: X -&gt; Y 라는 notation으로보면 정의역 X와 공역 Y가 있을 때, 정의역 X는 함수 f에 의해서 공역 중 하나로 대응되는 치역 f(x)을 만드는 것이다. 결국 선형변환을 함수로 보려면 선형변환의 정의역, 공역, 치역은 어느것이고 공역 중 하나로 대응될 수 있는가? 에 대한 해결을 하면 된다. 여기서 Gilbert 교수님의 유명한 row space, null space, column space, left null space 가 나오는데 정확히 이해 못한 부분이 있어서 다음 글로 넘긴다.(^^)다만 행렬이 선형함수이고, 자세하게는 행렬을 구성하는 행벡터가 선형함수는라는 것. 이 함수는 벡터의 특성을 가지기에 행벡터도 마찬가지로 벡터다!(행벡터인데 벡터가 아닌게 더 이상하지 않는가?)를 정확히 이해해야 위의 4개의 공간, 쌍대 공간으로 편하게 넘어갈 수 있는 것 같으니 이후에 한번 더 읽어봐야겠다.Hessian matrixHessian matrix 는 scalar-value를 갖는 다변수 함수의 2계 도함수(second-order)를 이용하여 만든 행렬이다. 위에 주르륵 적은 행렬은 데이터 자체 혹은 함수 로 보는데, hessian 은 어떤 함수가 가지는 방향을 표현하는 행렬이다. (갑자기 어떤 함수의 2계 도함수를 설명하는 것이라니.. 좀 어색할 수 있다.) 어쨌던 이 행렬은 2계 도함수로 표현된 행렬이기 때문에 정방행렬이고 symmetric 하다. 왜냐하면 함수가 f(x_1, x_2)로 정의될 때 \partial x_1, x_2를 구할 때 x_1으로 미분하고 x_2로 미분하나, x_2로 미분하고 x_1으로 하나 순서는 상관없다. 그리고 이렇게 2번 미분한걸 행렬로 표현하는거라 행과 열의 크기가 같기 때문이다.보통 square &amp; symmetric 행렬 A의 eigenvalue 값이 모두 양수면 positive-definite이라고 한다. positive definite의 정의는 여기 를 참고하되, 이게 중요한 이유는 함수가 볼록함수(convex)이냐 오목함수(concave)이냐를 판단하는 중요한 요소이기 때문이다.간단하게만 설명하자면 positive definite의 정의인 x^TAx&gt;0 는 x^T(Ax)&gt;0 이고, 이것은 벡터 x에 A를 곱한 결과 벡터와 x^T를 곱했을 때 0보다 커야한다는 것이다. 만약 A가 x를 변환할 때 90도 이상의 변환을 주지않는다면 Ax와 x^T를 내적하면 늘 양수값이 나오게 될거다. 늘 양수값이 나오는 함수가 뭐가 있을까? 간단하게 x^2+1를 생각할 수 있고, 예전에 배운 아래로 볼록 함수를 생각하면 된다! 볼록함수는 global minimum을 갖게 되는데, 양의 실수값만 가지는 2차 함수는 양의 무한 값을 가지지만 가장 작은 함수값을 가지는 점이 있지 않은가? 그런 꼴을 떠올리면 된다.Eigenvalue &amp; Eigenvectoreigenvalue와 eigenvector의 정의는 위에서 설명했지만 wikipedia와 linear algebra and its application 정의를 각각 보자.  In linear algebra, an eigenvector (/ˈaɪɡənˌvɛktər/) or characteristic vector of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. The corresponding eigenvalue, often denoted by {\displaystyle \lambda }\lambda , is the factor by which the eigenvector is scaled.  An eigenvector of an n by n matrix A is a nonzero vector x such that Ax = \lambda x for some scalar \lambda. A scalar \lambda is called an eigenvalue of A if there is a nontrivial solution x of Ax = \lambda x; such an x is called an eigenvector corresponding to \lambda그러니까 벡터에 행렬을 곱해줬는데 이 벡터가 방향은 안바뀌고 크기만 바뀌더라. 라는 소리다.Hessian &amp; Eigenvalue그럼 eigenvalue가 큰 행렬을 갖는다는 것은 뭘까? 어떤 행렬을 곱해줬을 때 방향은 안변할지라도 그 방향으로 변화가 크게 일어날 것이라는 말이다.반면에 eigenvalue가 작다는 것은 그 방향으로의 변화가 굉장히 작다는 말이 된다. Hessian 행렬이 어떤 함수의 변화도를 나타내는 행렬이라는걸 생각하면, 이 함수는 어떤 방향으로는 굉장히 가파르게 바뀌고 어떤 방향으로는 완만하게 변화한다고 생각할 수 있다.오늘 리뷰를 한 논문 “WHEN VISION TRANSFORMERS OUTPERFORM RESNETS WITHOUT PRE-TRAINING OR STRONG DATA AUGMENTATIONS” 은 ViT, MLP mixer 네트워크가 왜 많은 데이터를 통한 pre-training 또는 strong data augmentation을 주지 않고서는 generalization 성능이 ResNet보다 낮은지를 loss landscape으로 설명한다. Loss landscape visualization 논문에서 사용한 방식으로 시각화해서 보여주지만 이에 더불어 training loss의 2차 미분 정보를 담은 hessian 행렬의 max eigenvalue 값도 사용한다. 논문에서는 loss landscape 자체도 ViT, MLP mixer가 더 날카롭고, hessian의 eigenvalue 도 resnet보다 훨씬 큰 것을 미루어보다 함수의 형태(curvature)가 굉장히 날카로울 수 있다고 설명한다. 따라서 non-convex 한 딥러닝 모델에서 특히 sharp한 loss land scape을 가지는 경우 training loss 만을 줄이는 것이 아니라 smooth한 loss landscape을 가질 수 있도록 최적화를 고려해야하며, sharp aware minimization (SAM) optimizer를 사용해 pre-training, strong data augmentation 없이도 비슷한 파라미터 규모의 resnet과 비교해 높은 성능을 낼 수 있음을 보였다. 물론 그 이외에 초반 레이어를 sparse하게 만들어 pruning의 가능성을 높일 수 있고, attention map도 더 유의미하게 나온다는 장점도 보였으나 이는 생략한다. 이 논문 말고도 네이버에서 낸 How do vision transformers work 에서 마찬가지로 hessian으로 분석을 하는데, ViT와 resnet의 loss landscape 경향성이 좀 달라서 확인해봐야할 것 같다.끝으로글을 작성하며 러프하게 적은 부분도 있고, 헷갈리고 오류가 있는 부분도 있겠으나 오랜만에 공부하고 이해한 내용으로 정보성 글을 적으니 힘들지만 보람차긴하다. 오늘은 Hessian의 eigenvalue에 꽂혀서 PCA도 정리하려했는데, 정확히 어떻게 구현되는지 보려면 좀더 시간을 들여야겠다.Loss landscape 내용이나 second-order matrix는 한 2년전부터 코드 봐야지..봐야지.. 한거였는데, 조만간 시간 내어 코드 리뷰해봐야겠다.(제발)끝으로 시간 관계상 퇴고를 못했는데, 퇴고하면서 그림도 좀 끼워넣고 수식도 제대로 넣어야지 ~_~ 졸린다.참고  행렬곱에 대한 새로운 시각  행벡터의 의미와 벡터의 내적  4개 주요 부분 공간 간의 관계  positive definite]]></content>
      <categories>
        
          <category> linear_algebra/silly_questions </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Competition Mask Image Classification Preface]]></title>
      <url>/retro/classification_competition/2022/03/06/Competition-Mask-Image-Classification_Preface/</url>
      <content type="text"><![CDATA[시작하기에 앞서감정과 상황을 표현하는 글 쓰는 것을 즐겨하고 좋아했다. 아이러니하게도, 문학 작품에서 형용이 과한 글은 오히려 좋아하지 않았으면서 내 글은 주로 만연체가 되곤했다.그럼에도 불구하고, 내 생각을 표현하고 전개해나감에 있어서 긍정적인 평가를 꽤 받아왔다. 그리고, 그런 주위 반응에 상관없이 나는 공개한 내 글들을 좋아한다.나는 느끼한 글, 고민이 없는 글을 좋아하지 않는다. 그럼에도 불구하고, 많은 기술적 내용이 들어가야하는 전공 레포트, 논문의 글에서나는 내가 한 것을 포장해야하고, 고민한 마냥 적지만 소화하지 못한 글자를 쑤셔 넣는다. 그 비율이 어느정도이냐는 상관없다.내 것에 100%의 진심이나 자신감을 넣지 않았기 때문이지, 그 외의 다른 변명은 없다.그렇기에 정보성이 강한 글 중에는 내가 좋아하는, 나의 글이 없다.소프트웨어 엔지니어?소프트웨어 엔지니어 라는 단어가 나에게는 좀 불편하고 어려웠다. 무엇보다 내 인생 계획에 없었던 단어라 생각했다.그래서 친구들이 너 무슨 직군으로 지원하냐고 물어보면, 굳이 “AI 리서처나 엔지니어? 쪽으로 쓸 듯”이라고.. 내심 나 스스로에게 선을 그어 구분해서 말하곤 했다. 그쯤에 유튜브가 추천해준 구글 Research 엔지니어 인터뷰 영상과 조우했다. 대부분의 interviewee들이 “I’m a software engineer in google” 이라고 소개하고, 어떤 것을 연구하고 개발하는 지를 설명하더라.사실 내 눈 감고 있었던 거긴하지만.. 어쨌던 영상덕에 소연아..넌 소프트웨어 엔지니어 직군으로 가게 되는 것이다.를 받아드리게된..것…같다… 고맙다. 유튜브.일단, 소프트웨어 엔지니어의 길로 간다라는 결정을 내리고 걱정도, 무서운 것도, 쭈굴해지는 것도(..) 많았고요새.. 뭐 계속 정면 돌파하고 있다. 아파죽겠다. 그래도 좋다고 생각하는 것은 다른 어떤 분야보다도 치열하게 열심히 하는 사람들이가시적으로 눈에 많이 띈다는 것이다. 사실 공학의 어느 분야던 열심히 하는, 꾸준한, 대단한 사람들 정말 많다. 하지만 당장 시스템 칩을 만드는 엔지니어가 자신의 개발 과정을 소프트웨어 엔지니어만큼 기록하는 걸 보는 경우는 드물고,여러 내부적 상황 때문에 제공되는 정보도 제한적이다. 그리고 개인적인 생각으로는.. 학문의 진입 장벽이 엄청나기도 하다.반면, 소프트웨어 문화는 오픈 소스 선순환이 기저 철학이라 그런지 실제 제품, 즉 코드의 공유도 많고,시스템 제작 과정과 그 과정에서 발생하는 A-to-Z 회고도 공유된다. 매력적인 회고를주고 받는 문화를 가진 회사는 그 자체로 셀링포인트가 될 정도의 매력도를 뿜어내는 요즘.그런 회고 문화가 개인적으로 더 매력적으로 느껴지는 이유는 회고의 글은 느끼하지도 않고 고민도 엄청 나다.물론 내가 본 회고는 주로 대형 IT 기업쪽 글이라 그만큼 글의 질에 신경을 많이 썼겠지만, 여하튼 팩트와 독자를 고려한 글적 센스, 얼마간의 유머가 곁들어진 글은..매력적이다..!!!!한때, 나의 색이 무엇일까를 고민했고, 그 당시 주황색쯤이겠다라고 개인적 정리를 했더랬다. 한 5년이 지난 최근에는 색깔의 분류와는 상관없이 그 어느 색이던 그대로도 선명한 사람이기를 바란다.그렇다면 선명하다는게 무엇인가? 그리고 무엇에 선명해지고 싶은가? 꽤 치열하게 살아온 입장으로서 내가 하는 것의 만족감과 자아실현으로 인한 자신감으로부터 선명해지겠거니 한다. 그리고 가까운 2-3년 동안은 소프트웨어 엔지니어를 함으로써 그 선명함을 얻고 싶은거겠다. 라고 정리해본다.끝으로, 매력적인 글을 쓰는 소프트웨어 엔지니어라면 꽤 만족스러운 선명함일 것 같다.쓰고나니 좀 어려워보여서 마음이 무거워지는 기분이 든다.회고코딩을 수반하는 프로젝트를 하면서 회고라는 것을 해본 적이 없다. 애초에 코딩 프로젝트 경험이 많지도 않거니와 그마저도 협업 보다는 어떻게던 내가 맡은 부분이 돌아가게만 하면 됐기에 온갖 스파게티 양념 뿌려가면서 만들다보니.. 이미 불을만큼 불은 스파게티를 어디서부터 논해야하겠나. 그냥 안했고, 할 생각도 안했다. 그렇지만 인썸니아에서 PM으로 일할 때개발자들끼리 우루루 모여서 설계하고 코드 회고하는게 약간 간지나보이긴 했던 것 같다.지금은? 당장 과제로써 회고 제출 해야한다. 상황이 바뀐 것 말고도, 내심 석사기간동안 의미있게일을 마무리하고, 유의미한 피드백으로 매듭짓고 싶었던 마음을 실현할 수 있는 장치인 것 같다란 생각이 크다.무엇보다.. 이미지 분류 대회하면서 해볼 것, 해본 것, 망한 것, 잘된 것을 휘갈겨 쓴 노트보니까.. 이거 안하면 또 엉망진창 되겠구나란 생각이 스멀스멀와서..랄까? 써보고 싶어졌다. 잘 써보고 싶어졌다.끝으로회고로 시작은 하겠지만 여러모로 배운 것들을 소화하고 잘 정돈하는 글을 이어나갈 수 있도록 해보자.부스트캠프 글쓰기 모임을 이끌고 참여하시는 분들 덕에 동기부여를 얻어 긴 preface를 쓰게되어 감사하다.이어지는 회고의 글에서는 다음의 내용을 다루고자 한다.  내가 완전 망각했던 것  대회 하면서 헷갈렸던 것  해보고 싶었던 것 중 해본 것  해보고 싶었던 것 중 못 해본것  좋았던 다른 분들의 접근]]></content>
      <categories>
        
          <category> retro/classification_competition </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
